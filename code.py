# -*- coding: utf-8 -*-
"""Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r29jLfljzVjr40lSDUC6uJ9dLKjf_I2x
"""

# Standard scientific Python imports
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns # for visualisation
import numpy as np

#load data
y_train = pd.read_csv('y_train.csv', index_col = 0, squeeze=True)
X_train = pd.read_csv('X_train.csv', index_col = 0, header=[0, 1, 2])
X_test = pd.read_csv( 'X_test.csv', index_col = 0, header=[0, 1, 2])


##check for columns contains single/few values
X_train.loc[:,X_train.nunique()<=1].columns #all have at least 2 values
X_train.nunique().sort_values(ascending=True).head(n=30)

# get number of unique values for each column
counts = X_train.nunique()

# record columns to delete : 24 columns--cqt_max, stft_max
to_del = [i for i,v in enumerate(counts) if (float(v)/X_train.shape[0]*100) < 1] #cqt_max, stft_max
#print(counts.sort_values(ascending=True).head(n=24))
#X_train.columns[to_del]


#duplicate rows
sum(X_train.duplicated()) #13 duplicate rows: remove
X_train_remove=X_train[X_train.duplicated()==False]

#final data we use:clean data
X_train_clean=X_train_remove.drop(X_train.columns[to_del],axis=1) #5987*494
y_train_clean=y_train[X_train.duplicated()==False] #5987*1
X_test_clean=X_test.drop(X_train.columns[to_del],axis=1) #2000*494

#Cross validation on Neurual Network
#use MLPClassifier
from sklearn.model_selection import cross_val_score
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler # Data should be standaidised based on different training set in CV, pipeline solves the problem
from sklearn.pipeline import Pipeline  

#Example: Tune alpha in [0.1,1,10]

alpha_all = np.array([0.1,1,10])
nfold=5  #5 fold cross validation
scalar = StandardScaler()
#model_list_nn=[]
risk_cv_nn= np.zeros(shape=alpha_all.shape)
for (i, a) in enumerate(alpha_all):
    print('alpha=', a)
    clf_nn = MLPClassifier(hidden_layer_sizes = (256,), activation='relu',alpha=a, max_iter=300,batch_size = 32,verbose=True,solver='adam')
    pipeline = Pipeline([('transformer', scalar), ('estimator', clf_nn)]) #scale data before fitting model each time
    risk_cv_nn[i] = 1-(cross_val_score(pipeline, X_train_clean,y_train_clean, cv = nfold)).mean() #save cross validation loss for each value
    #loss, not accuracy here. if want accuracy: 1-risk_cv_nn
    
plt.figure()
plt.plot(alpha_all, risk_cv_nn) 
plt.xlabel('alpha')
plt.ylabel('Validation risk');

print(risk_cv_nn)

# code from lecture:

# Train-test split 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)
print('Training:', X_train.shape)
print('Test:', X_test.shape)

# Scale
scaler = StandardScaler()
scaler.fit(X_train)
X_train_sc = scaler.transform(X_train)
X_test_sc = scaler.transform(X_test)

#tune batch size 
batch_all = np.array([1, 32, 100, X_train.shape[0]])

plt.figure()
for batch_size in batch_all:
    print('Batch size = ', batch_size)
    clf = MLPClassifier(hidden_layer_sizes = (50,), activation='tanh', max_iter=2000, solver='sgd', batch_size = batch_size)
    clf.fit(X_train_sc, y_train)
    plt.plot(clf.loss_curve_)
plt.xlabel('Epoch')
plt.ylabel('Objective function');
plt.legend(['nb=1', 'nb=32', 'nb=100', 'nb=n']);
#loss curve is plotted

#if you want to save the whole workspace:
import dill
dill.dump_session('notebook_env_nn.db')
#to load the workspace, just use 
dill.load_session('notebook_env_nn.db') #everything will be there